<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>马尔可夫 - 强化学习 | Northboat House</title>
    <meta name="generator" content="VuePress 1.9.7">
    <link rel="icon" href="/img/boat.ico">
    <link rel="stylesheet" href="/css/katex0.15.1.min.css">
    <script src="/js/katex0.15.1.min.js"></script>
    <script src="/js/jquery1.9.1.min.js"></script>
    <script src="/js/api.js"></script>
    <script src="/js/crypto.js"></script>
    <script src="/js/md5.js"></script>
    <script src="/js/clock.js"></script>
    <link rel="stylesheet" href="/css/button.css">
    <meta name="description" content="Docs and Blog through My Student Period.">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/assets/css/0.styles.9ca413d0.css" as="style"><link rel="preload" href="/assets/js/app.314cf608.js" as="script"><link rel="preload" href="/assets/js/10.141c7d77.js" as="script"><link rel="preload" href="/assets/js/1.e7a7181a.js" as="script"><link rel="preload" href="/assets/js/176.af70a6e0.js" as="script"><link rel="prefetch" href="/assets/js/100.d9b784d7.js"><link rel="prefetch" href="/assets/js/101.66fc70d1.js"><link rel="prefetch" href="/assets/js/102.5f0d8f05.js"><link rel="prefetch" href="/assets/js/103.9ef30057.js"><link rel="prefetch" href="/assets/js/104.33db21d9.js"><link rel="prefetch" href="/assets/js/105.e5dd4a15.js"><link rel="prefetch" href="/assets/js/106.8f166734.js"><link rel="prefetch" href="/assets/js/107.b104c021.js"><link rel="prefetch" href="/assets/js/108.3fce06df.js"><link rel="prefetch" href="/assets/js/109.b230ac61.js"><link rel="prefetch" href="/assets/js/11.688df8d9.js"><link rel="prefetch" href="/assets/js/110.c0722f07.js"><link rel="prefetch" href="/assets/js/111.d43f6043.js"><link rel="prefetch" href="/assets/js/112.e2707e13.js"><link rel="prefetch" href="/assets/js/113.cf85417c.js"><link rel="prefetch" href="/assets/js/114.26959b6a.js"><link rel="prefetch" href="/assets/js/115.827529bd.js"><link rel="prefetch" href="/assets/js/116.da6f8719.js"><link rel="prefetch" href="/assets/js/117.3ed31b19.js"><link rel="prefetch" href="/assets/js/118.e87909aa.js"><link rel="prefetch" href="/assets/js/119.b4781208.js"><link rel="prefetch" href="/assets/js/12.d6a7a2eb.js"><link rel="prefetch" href="/assets/js/120.4677f5db.js"><link rel="prefetch" href="/assets/js/121.e9ae8847.js"><link rel="prefetch" href="/assets/js/122.97151d48.js"><link rel="prefetch" href="/assets/js/123.c7f5f6cf.js"><link rel="prefetch" href="/assets/js/124.b8a6eb72.js"><link rel="prefetch" href="/assets/js/125.3f5386ca.js"><link rel="prefetch" href="/assets/js/126.439c01af.js"><link rel="prefetch" href="/assets/js/127.618a3718.js"><link rel="prefetch" href="/assets/js/128.22a1b68d.js"><link rel="prefetch" href="/assets/js/129.d8293c12.js"><link rel="prefetch" href="/assets/js/13.b19e874b.js"><link rel="prefetch" href="/assets/js/130.42444747.js"><link rel="prefetch" href="/assets/js/131.e6e1f524.js"><link rel="prefetch" href="/assets/js/132.cb61877f.js"><link rel="prefetch" href="/assets/js/133.5393a860.js"><link rel="prefetch" href="/assets/js/134.932ca7b3.js"><link rel="prefetch" href="/assets/js/135.d249b3ea.js"><link rel="prefetch" href="/assets/js/136.8a0d291c.js"><link rel="prefetch" href="/assets/js/137.8facf127.js"><link rel="prefetch" href="/assets/js/138.b9ada001.js"><link rel="prefetch" href="/assets/js/139.e0c529fd.js"><link rel="prefetch" href="/assets/js/14.7e9a1e96.js"><link rel="prefetch" href="/assets/js/140.99160496.js"><link rel="prefetch" href="/assets/js/141.01dcf4be.js"><link rel="prefetch" href="/assets/js/142.8197d18f.js"><link rel="prefetch" href="/assets/js/143.31429023.js"><link rel="prefetch" href="/assets/js/144.b1404ad6.js"><link rel="prefetch" href="/assets/js/145.511e9c8d.js"><link rel="prefetch" href="/assets/js/146.6887aa13.js"><link rel="prefetch" href="/assets/js/147.287796d9.js"><link rel="prefetch" href="/assets/js/148.6bd69944.js"><link rel="prefetch" href="/assets/js/149.ba225599.js"><link rel="prefetch" href="/assets/js/15.416255d8.js"><link rel="prefetch" href="/assets/js/150.4203ba06.js"><link rel="prefetch" href="/assets/js/151.8da14070.js"><link rel="prefetch" href="/assets/js/152.887425ba.js"><link rel="prefetch" href="/assets/js/153.1a5de2d2.js"><link rel="prefetch" href="/assets/js/154.a6a26d5a.js"><link rel="prefetch" href="/assets/js/155.f4d1112f.js"><link rel="prefetch" href="/assets/js/156.74f1629e.js"><link rel="prefetch" href="/assets/js/157.ae976a5e.js"><link rel="prefetch" href="/assets/js/158.f19b607b.js"><link rel="prefetch" href="/assets/js/159.73dbc4b1.js"><link rel="prefetch" href="/assets/js/16.964cdc8b.js"><link rel="prefetch" href="/assets/js/160.bab471d9.js"><link rel="prefetch" href="/assets/js/161.16ee793c.js"><link rel="prefetch" href="/assets/js/162.c1dde357.js"><link rel="prefetch" href="/assets/js/163.9b612500.js"><link rel="prefetch" href="/assets/js/164.c87caf8f.js"><link rel="prefetch" href="/assets/js/165.7a146cd9.js"><link rel="prefetch" href="/assets/js/166.24500cd7.js"><link rel="prefetch" href="/assets/js/167.537e27f8.js"><link rel="prefetch" href="/assets/js/168.0b72e056.js"><link rel="prefetch" href="/assets/js/169.c488fccd.js"><link rel="prefetch" href="/assets/js/17.f7816437.js"><link rel="prefetch" href="/assets/js/170.52532684.js"><link rel="prefetch" href="/assets/js/171.89292e5e.js"><link rel="prefetch" href="/assets/js/172.c947977f.js"><link rel="prefetch" href="/assets/js/173.b3d519ef.js"><link rel="prefetch" href="/assets/js/174.3a3e65f5.js"><link rel="prefetch" href="/assets/js/175.19977dbf.js"><link rel="prefetch" href="/assets/js/177.2b823869.js"><link rel="prefetch" href="/assets/js/178.cc512f1b.js"><link rel="prefetch" href="/assets/js/179.23cc7703.js"><link rel="prefetch" href="/assets/js/18.c9a6bad8.js"><link rel="prefetch" href="/assets/js/180.f0f21069.js"><link rel="prefetch" href="/assets/js/181.8fe2d5cf.js"><link rel="prefetch" href="/assets/js/182.92c65ada.js"><link rel="prefetch" href="/assets/js/183.29a8ea33.js"><link rel="prefetch" href="/assets/js/184.68da2f77.js"><link rel="prefetch" href="/assets/js/185.dd6c7102.js"><link rel="prefetch" href="/assets/js/186.67690be0.js"><link rel="prefetch" href="/assets/js/187.18ddff24.js"><link rel="prefetch" href="/assets/js/188.0b1f8b57.js"><link rel="prefetch" href="/assets/js/189.6ba9aeea.js"><link rel="prefetch" href="/assets/js/19.1857262e.js"><link rel="prefetch" href="/assets/js/190.c2519d23.js"><link rel="prefetch" href="/assets/js/191.e7468dfb.js"><link rel="prefetch" href="/assets/js/192.811ec949.js"><link rel="prefetch" href="/assets/js/193.86dd8cbc.js"><link rel="prefetch" href="/assets/js/20.c39585ed.js"><link rel="prefetch" href="/assets/js/21.37bf76ea.js"><link rel="prefetch" href="/assets/js/22.ad5c92cb.js"><link rel="prefetch" href="/assets/js/23.bac896f4.js"><link rel="prefetch" href="/assets/js/24.f174f171.js"><link rel="prefetch" href="/assets/js/25.42e74948.js"><link rel="prefetch" href="/assets/js/26.a26f70dd.js"><link rel="prefetch" href="/assets/js/27.b1a135f6.js"><link rel="prefetch" href="/assets/js/28.b3eb51c9.js"><link rel="prefetch" href="/assets/js/29.d0445e8e.js"><link rel="prefetch" href="/assets/js/3.a185479d.js"><link rel="prefetch" href="/assets/js/30.8068a5a3.js"><link rel="prefetch" href="/assets/js/31.56e6de09.js"><link rel="prefetch" href="/assets/js/32.70b22f5e.js"><link rel="prefetch" href="/assets/js/33.df71b712.js"><link rel="prefetch" href="/assets/js/34.d10eb5cf.js"><link rel="prefetch" href="/assets/js/35.c8ce52b1.js"><link rel="prefetch" href="/assets/js/36.7c3caf6b.js"><link rel="prefetch" href="/assets/js/37.eb002b54.js"><link rel="prefetch" href="/assets/js/38.d45b4c47.js"><link rel="prefetch" href="/assets/js/39.aa838e94.js"><link rel="prefetch" href="/assets/js/4.d9e0abb6.js"><link rel="prefetch" href="/assets/js/40.da060e32.js"><link rel="prefetch" href="/assets/js/41.ea78fea8.js"><link rel="prefetch" href="/assets/js/42.822ad732.js"><link rel="prefetch" href="/assets/js/43.95a91fdd.js"><link rel="prefetch" href="/assets/js/44.4d7be59b.js"><link rel="prefetch" href="/assets/js/45.6b3c75ff.js"><link rel="prefetch" href="/assets/js/46.54ed0dca.js"><link rel="prefetch" href="/assets/js/47.e38e5992.js"><link rel="prefetch" href="/assets/js/48.5e8a3ee9.js"><link rel="prefetch" href="/assets/js/49.71cf5a97.js"><link rel="prefetch" href="/assets/js/5.fda6dc86.js"><link rel="prefetch" href="/assets/js/50.a138d0fb.js"><link rel="prefetch" href="/assets/js/51.b35223be.js"><link rel="prefetch" href="/assets/js/52.4e0910f5.js"><link rel="prefetch" href="/assets/js/53.0529c473.js"><link rel="prefetch" href="/assets/js/54.77dad0f8.js"><link rel="prefetch" href="/assets/js/55.84bca52a.js"><link rel="prefetch" href="/assets/js/56.f37e5308.js"><link rel="prefetch" href="/assets/js/57.2d3ac42e.js"><link rel="prefetch" href="/assets/js/58.92ef5bb4.js"><link rel="prefetch" href="/assets/js/59.e66fe24c.js"><link rel="prefetch" href="/assets/js/6.7587c924.js"><link rel="prefetch" href="/assets/js/60.a09cd24b.js"><link rel="prefetch" href="/assets/js/61.1d0ac78d.js"><link rel="prefetch" href="/assets/js/62.0d91b25f.js"><link rel="prefetch" href="/assets/js/63.7c42ef49.js"><link rel="prefetch" href="/assets/js/64.ef4a5371.js"><link rel="prefetch" href="/assets/js/65.1838ddfa.js"><link rel="prefetch" href="/assets/js/66.73e341d2.js"><link rel="prefetch" href="/assets/js/67.01b02451.js"><link rel="prefetch" href="/assets/js/68.26a8bb57.js"><link rel="prefetch" href="/assets/js/69.d8181f52.js"><link rel="prefetch" href="/assets/js/7.3e38d36a.js"><link rel="prefetch" href="/assets/js/70.fa7b74fc.js"><link rel="prefetch" href="/assets/js/71.187e6e51.js"><link rel="prefetch" href="/assets/js/72.36e29808.js"><link rel="prefetch" href="/assets/js/73.8a7aefa6.js"><link rel="prefetch" href="/assets/js/74.670559cc.js"><link rel="prefetch" href="/assets/js/75.dd10f6c8.js"><link rel="prefetch" href="/assets/js/76.a1825cfc.js"><link rel="prefetch" href="/assets/js/77.abe0ae70.js"><link rel="prefetch" href="/assets/js/78.c84999ef.js"><link rel="prefetch" href="/assets/js/79.7028782d.js"><link rel="prefetch" href="/assets/js/8.41e4f2a8.js"><link rel="prefetch" href="/assets/js/80.d12affc9.js"><link rel="prefetch" href="/assets/js/81.024c4f3e.js"><link rel="prefetch" href="/assets/js/82.024f448a.js"><link rel="prefetch" href="/assets/js/83.4d9d14b7.js"><link rel="prefetch" href="/assets/js/84.449c2caf.js"><link rel="prefetch" href="/assets/js/85.d1ab1c5d.js"><link rel="prefetch" href="/assets/js/86.bd6b8a80.js"><link rel="prefetch" href="/assets/js/87.a498c489.js"><link rel="prefetch" href="/assets/js/88.fb87ca7f.js"><link rel="prefetch" href="/assets/js/89.c526090b.js"><link rel="prefetch" href="/assets/js/9.dfb49811.js"><link rel="prefetch" href="/assets/js/90.f72ed1a8.js"><link rel="prefetch" href="/assets/js/91.2b8774c5.js"><link rel="prefetch" href="/assets/js/92.b3890454.js"><link rel="prefetch" href="/assets/js/93.c51681c8.js"><link rel="prefetch" href="/assets/js/94.3a0b704a.js"><link rel="prefetch" href="/assets/js/95.fc027736.js"><link rel="prefetch" href="/assets/js/96.db25d28f.js"><link rel="prefetch" href="/assets/js/97.63c4bba8.js"><link rel="prefetch" href="/assets/js/98.554d153a.js"><link rel="prefetch" href="/assets/js/99.7ecfa094.js">
    <link rel="stylesheet" href="/assets/css/0.styles.9ca413d0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-7dd95ae2><div data-v-7dd95ae2><div class="password-shadow password-wrapper-out" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>Northboat House</h3> <p class="description" data-v-59e6cb88>Docs and Blog through My Student Period.</p> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>northboat</span>
          
        <span data-v-59e6cb88>2021 - </span>
        2024
      </a></span></div></div> <div class="hide" data-v-7dd95ae2><header class="navbar" data-v-7dd95ae2><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/leaf.ico" alt="Northboat House" class="logo"> <span class="site-name">Northboat House</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      Home
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/" class="nav-link"><i class="iconfont reco-blog"></i>
  Root
</a></li><li class="dropdown-item"><!----> <a href="/docs/lib/" class="nav-link"><i class="iconfont reco-blog"></i>
  Library
</a></li><li class="dropdown-item"><!----> <a href="/categories/Blog/" class="nav-link"><i class="iconfont reco-blog"></i>
  Blog
</a></li><li class="dropdown-item"><!----> <a href="https://github.com/northboat" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-blog"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-github"></i>
      2084 DevOps
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/dev/algo/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Algorithm
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/java/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/op/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Operations
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/fe/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Front End
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      NEUQ CS
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/cs/cn/" class="nav-link"><i class="iconfont reco-category"></i>
  Computer Network
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/ds/" class="nav-link"><i class="iconfont reco-category"></i>
  Data Structure
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/co/" class="nav-link"><i class="iconfont reco-category"></i>
  Computer Organization
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/os/" class="nav-link"><i class="iconfont reco-category"></i>
  Operating System
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-other"></i>
      XDU CyberSec
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/sec/fml/" class="nav-link router-link-active"><i class="iconfont reco-eye"></i>
  Federated ML
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/math/" class="nav-link"><i class="iconfont reco-eye"></i>
  Math
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/crypto/" class="nav-link"><i class="iconfont reco-eye"></i>
  Cryptography
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/pen/" class="nav-link"><i class="iconfont reco-eye"></i>
  Penetration
</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-7dd95ae2></div> <aside class="sidebar" data-v-7dd95ae2><div class="personal-info-wrapper" data-v-1fad0c41 data-v-7dd95ae2><img src="/img/boat.ico" alt="author-avatar" class="personal-img" data-v-1fad0c41> <h3 class="name" data-v-1fad0c41>
    northboat
  </h3> <div class="num" data-v-1fad0c41><div data-v-1fad0c41><h3 data-v-1fad0c41>170</h3> <h6 data-v-1fad0c41>Articles</h6></div> <div data-v-1fad0c41><h3 data-v-1fad0c41>22</h3> <h6 data-v-1fad0c41>Tags</h6></div></div> <ul class="social-links" data-v-1fad0c41></ul> <hr data-v-1fad0c41></div> <nav class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-home"></i>
      Home
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/" class="nav-link"><i class="iconfont reco-blog"></i>
  Root
</a></li><li class="dropdown-item"><!----> <a href="/docs/lib/" class="nav-link"><i class="iconfont reco-blog"></i>
  Library
</a></li><li class="dropdown-item"><!----> <a href="/categories/Blog/" class="nav-link"><i class="iconfont reco-blog"></i>
  Blog
</a></li><li class="dropdown-item"><!----> <a href="https://github.com/northboat" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-blog"></i>
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-github"></i>
      2084 DevOps
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/dev/algo/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Algorithm
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/java/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/op/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Operations
</a></li><li class="dropdown-item"><!----> <a href="/docs/dev/fe/" class="nav-link"><i class="iconfont reco-bokeyuan"></i>
  Front End
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-document"></i>
      NEUQ CS
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/cs/cn/" class="nav-link"><i class="iconfont reco-category"></i>
  Computer Network
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/ds/" class="nav-link"><i class="iconfont reco-category"></i>
  Data Structure
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/co/" class="nav-link"><i class="iconfont reco-category"></i>
  Computer Organization
</a></li><li class="dropdown-item"><!----> <a href="/docs/cs/os/" class="nav-link"><i class="iconfont reco-category"></i>
  Operating System
</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-other"></i>
      XDU CyberSec
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/docs/sec/fml/" class="nav-link router-link-active"><i class="iconfont reco-eye"></i>
  Federated ML
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/math/" class="nav-link"><i class="iconfont reco-eye"></i>
  Math
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/crypto/" class="nav-link"><i class="iconfont reco-eye"></i>
  Cryptography
</a></li><li class="dropdown-item"><!----> <a href="/docs/sec/pen/" class="nav-link"><i class="iconfont reco-eye"></i>
  Penetration
</a></li></ul></div></div> <!----></nav> <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Python3</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>人工智能导论</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/docs/sec/fml/cs188/cs188-search-csp-game.html" class="sidebar-link">搜索 - 约束满足 - 博弈</a></li><li><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html" aria-current="page" class="active sidebar-link">马尔可夫 - 强化学习</a></li><li><a href="/docs/sec/fml/cs188/cs188-probabilistic-reasoning.html" class="sidebar-link">不确定知识 - 概率推理</a></li><li><a href="/docs/sec/fml/cs188/cs188-hmm-ml.html" class="sidebar-link">隐马尔科夫 - 机器学习</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>机器学习导论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>深度学习库 &amp; 框架</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-59e6cb88 data-v-7dd95ae2><h3 class="title" data-v-59e6cb88>马尔可夫 - 强化学习</h3> <!----> <label id="box" class="inputBox" data-v-59e6cb88><input type="password" value="" data-v-59e6cb88> <span data-v-59e6cb88>Konck! Knock!</span> <button data-v-59e6cb88>OK</button></label> <div class="footer" data-v-59e6cb88><span data-v-59e6cb88><i class="iconfont reco-theme" data-v-59e6cb88></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-59e6cb88>vuePress-theme-reco</a></span> <span data-v-59e6cb88><i class="iconfont reco-copyright" data-v-59e6cb88></i> <a data-v-59e6cb88><span data-v-59e6cb88>northboat</span>
          
        <span data-v-59e6cb88>2021 - </span>
        2024
      </a></span></div></div> <div data-v-7dd95ae2><div data-v-7dd95ae2><main class="page"><section style="display:;"><div class="page-title"><h1 class="title">马尔可夫 - 强化学习</h1> <div data-v-8a445198><i class="iconfont reco-account" data-v-8a445198><span data-v-8a445198>northboat</span></i> <i class="iconfont reco-date" data-v-8a445198><span data-v-8a445198>4/24/2022</span></i> <!----> <i class="tags iconfont reco-tag" data-v-8a445198><span class="tag-item" data-v-8a445198>AI</span></i></div></div> <div class="theme-reco-content content__default"><h2 id="mdps">MDPs</h2> <blockquote><p>Markov Decision Processes</p> <p>马尔可夫决定过程</p></blockquote> <h3 id="what-is-mdps">What is MDPs</h3> <p>进程：对搜索的概括</p> <p>计算可能的结果</p> <p>在<code>GridWorld</code>中，你决定向北走（因为这是最佳策略），但可能会执行失败（撞墙）</p> <p>MDP：Reward ——&gt; 结果</p> <ul><li>happy reward</li> <li>bad reward</li></ul> <p>目标很松散 ——&gt;为了最大化奖励的总和</p> <p>An MDP is defined by</p> <ul><li><p>a set of states s</p></li> <li><p>a set of actions a</p></li> <li><p>a transition function T(s, a, s')</p> <ul><li><p>Probability that a from s leads to s', called P(s'| s, a)</p> <p>在s状态执行a行为到达s'的代价</p></li> <li><p>Also called the model or the dynamics</p> <p>不同于搜索，这个后继函数有很多个，如在每个地点都可以向东南西北移动</p></li></ul></li> <li><p>a reward function R(s, a, s')</p> <ul><li><p>sometimes just R(s) or R(s')</p> <p>奖惩制度，有时只取决起点或终点</p></li></ul></li> <li><p>a start state</p></li> <li><p>maybe a terminal state</p></li></ul> <p>MDPs是不确定性搜索问题</p> <ul><li>强化学习的基础</li></ul> <p>expectimax（最大期望算法）算法可以MDP问题</p> <p>action outcomes depend on</p> <ul><li>未来要到达的状态</li> <li>你要执行的行动</li></ul> <p>MDPs适合嘈杂的世界</p> <p><strong>Grid World</strong></p> <p>Policy：策略</p> <ul><li>通过状态告诉你动作的功能</li> <li>如在地图上每个点标好你该往哪个方向走</li></ul> <p>optimal policy：最优策略</p> <ul><li>或许存在很多等效策略</li></ul> <p>competition</p> <ul><li>移动奖励（负0.1）是那么微不足道而不值得冒险去坑附近</li> <li>宁愿什么都不做，也不愿犯错</li></ul> <p>当移动代价变得更大，策略将更倾向与冒险在坑附近</p> <p>当更更大时，甚至有可能直接跳坑而避免移动花销</p> <p><strong>Racing</strong></p> <p>states：</p> <ul><li>cool</li> <li>warm</li> <li>overheated：risk the danger of breaking</li></ul> <p>跟据当前的温度决定是加速还是减速</p> <p><strong>Racing Search Tree</strong></p> <blockquote><p>tool：epectimax search</p></blockquote> <p>actions：</p> <ul><li>slower</li> <li>faster</li></ul> <p>state：</p> <ul><li>warm</li> <li>cool</li> <li>over heated</li></ul> <p>这棵树是无限的</p> <ul><li>Q state：选择了但还没行动的过度状态</li></ul> <p><strong>Utilities of Sequences</strong></p> <p>实用程序的选择顺序</p> <ul><li>more or less</li> <li>now or later</li></ul> <p>隐含的权衡</p> <p><strong>discount</strong></p> <p>对奖励的贬值，对晚来的价值施以惩罚，如每走一步，未得到的价值便腐朽0.8，0.8便是折扣</p> <p>当折扣越大，即<code>λ</code>越小，agent将变得越贪婪，越在意眼前的价值，而不是以后获得更大的利益</p> <p><strong>Preferences</strong></p> <p>假设偏好是固定的</p> <p>two ways to define utilities</p> <ul><li>additive utility</li> <li>discounted utility</li></ul> <p>如何处理无限的问题</p> <ul><li>Finite horizen：similar to depth-limited search，即限定树的深度</li> <li>Discounting：价值总是贬值，将无限接近于0</li> <li>Absorbing state：使用一系列终止状态，即</li></ul> <p>Markov decision processes：</p> <ul><li>状态集</li> <li>初始状态</li> <li>行为集</li> <li>过渡函数：提供的是概率</li> <li>奖惩机制</li></ul> <p>它的输出是每个state上对应的action，他实际上并没有真正在试错，而是去给每个状态分配最佳的行动，这就是MDP</p> <h3 id="solving-mdps">Solving MDPs</h3> <p>Quantities：</p> <ul><li>Policy：map of states of actions</li> <li>Utility：sum of discounted reward</li> <li>Values：expected future utility from a state（max node）</li> <li>Q Values：expected future from a q-state（chance node）</li></ul> <p>Optimal Quantities</p> <ul><li>V(s)*：状态的期望值（或许是平均值）</li> <li>Q*(s, a)：在状态s执行动作a后起的最佳作用</li> <li>P*(s)：当前状态的最佳策略（算法产出）</li></ul> <p>expectimax search可以解决这一问题，估算价值，选出最大价值，赋值</p> <p>考虑一下其他的算法</p> <ul><li><p><code>V*(s) = maxQ*(s, a)</code></p> <p>虽然Q*(s, a)还不知道怎么算</p></li> <li><p><code>Q*(s, a) = avg(sum(R(s, a, s') + λV*(s'))</code></p> <p>这是一个递归的定义，因为你并不知道V*(s')直到搜索到终点</p></li></ul> <p>此之谓贝尔曼方程：Bellman Equations</p> <ul><li>take correct first action</li> <li>kepp being optimal</li></ul> <p>回顾一下Racing Search Tree</p> <p>他是无限的，并且只有三种状态，如果用expectimax search，将会有指数级的重复工作（子树）</p> <h4 id="value-iteration">Value Iteration</h4> <p>价值迭代算法</p> <ul><li>from the bottom（deep enough）, recur the top</li> <li><code>V*(s) = maxΣT(s,a,s')[R(s,a,s') + λV*(s')]</code></li></ul> <p>利用贝尔曼方程确实可以搜索到底部并且递归回顶部，在这个递归过程中，各节点的值是不断更新的，且更加准确，直到保持稳定，即递归完毕</p> <ul><li>这个收敛的过程称作<code>bellman update</code></li></ul> <p><strong>Computing Time-Limited Values</strong></p> <p>对于一颗无限树，采用时间限制其递归深度，令V*(s)尽可能准确</p> <p>因为条件有限，我们无法完整进行贝尔曼算法，即只能尽可能的接近V*(s)</p> <ul><li><code>Vk(s) = avg(sum(R(s, a, s') + λVk(s')))</code></li></ul> <p>其中<code>Vk(s)、Vk(s')</code>都取其均值</p> <ul><li>take average</li> <li>像一个单层的expectimax搜索，但不同的是，他会由于递归深度的增加不断调整Vk值</li></ul> <p><strong>Convergence</strong></p> <p>VK compute</p> <p>一个k层树和一个k+1层树</p> <p>由于搜索深度增加，对于未来某节点的折扣也增加，也就是说越往后对总值的影响应是越小，细微调整</p> <p>当discount&gt;=1，没有趋同保证</p> <h4 id="policy-evaluation">Policy Evaluation</h4> <p>策略评估方法</p> <h5 id="fixed-policies">fixed Policies</h5> <p>固定的策略</p> <ul><li>do the optimal action</li> <li>do what Pi says</li> <li>easier than the optimal</li></ul> <p>假设你的固定策略选出的后继节点是最佳的</p> <ul><li><code>VΠ(s) = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></li></ul> <p>固定策略例如：一直向右走；一直向前走</p> <p>列举所有策略，评估所有策略，选择得分最高的策略</p> <h5 id="policy-evaluation-2">policy evaluation</h5> <p>输入一个策略，执行策略，得到该策略的值向量</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <h5 id="policy-extraction">Policy Extraction</h5> <p>即使当找到了相邻的最佳值，仍然要做一次expectimax去找到导致这个最佳值的行动</p> <ul><li>从值中找出行动，以更新策略</li></ul> <p>价值驱动决策</p> <p>Computing Actions from Q-Values</p> <h4 id="policy-iteration">Policy Iteration</h4> <p>价值迭代的问题</p> <ul><li>每次迭代将会耗费<code>O(s^2*A)</code>，这很慢</li> <li>每个状态的最大值很少改变，这意味着做了很多低效工作</li></ul> <p>正确策略下的无用选择 ——&gt; 错误的策略试错</p> <p>我们采用策略迭代</p> <ul><li>首先选择一些策略，并执行他们，估算状态价值</li> <li>改善你的策略，再次考虑之前的行动，重复估值</li> <li>直到策略收敛</li></ul> <p>可以证明它是最佳且收敛的，并且在很多情况比价值迭代收敛得更快</p> <p>VΠ是由当前策略得到的当前“最佳值”</p> <p><code>VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]</code></p> <p>根据这个当前最佳值，更新上一步的策略，比如我上一步策略原来是往北走，但这个最佳值得往东走，那么我更新上一步的策略为向东走</p> <ul><li>MDPs本质上便是找到每步的最佳策略，值迭代同时考虑策略和价值，在每步做出最佳选择；策略迭代通过值去找到更优的策略</li> <li>二者都是迭代，从叶子回溯到顶部</li></ul> <p>通常根据最后值的变化来确定是否已经收敛</p> <h3 id="summary">Summary</h3> <ul><li>compute optimal values：both can</li> <li>compute values for partivular policy：policy evaluation（策略评估）</li> <li>turn your values into policy：use policy extraction（策略抽取）</li></ul> <p>通常Policy Iteration是policy evaluation和policy improvement交替执行直到收敛</p> <p>Value Iteration是寻找Optimal value function和执行一次policy extraction</p> <ul><li>均属于动态规划算法</li></ul> <p><strong>Double-Bandit MDP</strong></p> <p>两台老虎机，一台（blue）拉一次给一块钱；另一台（red）拉一次给0元或2元。共拉一百次</p> <p>更优的策略？</p> <ul><li><code>red one</code>获得2元的概率为0.75</li></ul> <p>平均上</p> <ul><li>blue：100元</li> <li>red：150元</li></ul> <p>当获得2元的概率未知，尝试red one去获得信息</p> <p>core of reinfocement Learning：exploraton</p> <p>只能探索才能获取更多信息</p> <p>pay for the infomation and get return</p> <p>甚至不需要MDP算法，只需要不断探索和基本的数学直觉，试出概率</p> <h2 id="rl">RL</h2> <blockquote><p>Reinforcement learning：强化学习</p> <p>It's about how to learn behaviors</p></blockquote> <ul><li>Agent —actions—&gt; Environment</li> <li>Environment —state/reward—&gt; Agent</li></ul> <p>Agent和Environment都是动态变化的</p> <p>Basic idea：</p> <ul><li>agent接收奖惩反馈</li> <li>奖惩函数决定agent的效用</li> <li>为了最大化奖励，必须去学习最优行动</li> <li>所有的学习基于观察样例后的结果</li></ul> <p>learning rather than plan</p> <p>Examples：</p> <ul><li>Robot dog learning to Walk</li> <li>Snake rebot sidewingding（爬墙）</li></ul> <p>因为真实世界的规则并不是确定的，难以建模，这时让程序根据概率学习正确的行为显得更加高效</p> <ul><li><p>Toddler Robot（幼儿机器人）</p> <p>know how to stand after fall down</p></li></ul> <p>机器学习的最开始，他是不知道怎么做的，只是来回摆动，因为他不知道怎么获取奖励，于是开始瞎几把试，当偶然获取奖励后，他将根据奖惩制度完善自己的行动策略，从而行动得更加高效</p> <p>Still assume a Markov decision process</p> <ul><li><p>a set of states</p></li> <li><p>a set of actions</p></li> <li><p>a model T(s,a,s')</p> <p>原为 a successor function T(s,a,s')</p></li> <li><p>a reward function R(s,a,s')</p></li></ul> <p>Still looking for a policy</p> <p>The defference：We don't know T or R</p> <ul><li><p>不知道哪个状态是好的或哪个动作是好的</p> <p>就像那个老虎机不知道掉落概率</p></li> <li><p>必须真正去行动和访问状态去学习，去获取必要信息</p></li></ul> <p>Offline（MDPs） vs. Online（RL）</p> <ul><li>Offline Solution</li> <li>Online Learning</li></ul> <h3 id="model-based-learning">Model-Based Learning</h3> <p>Basic idea：</p> <ul><li>learn an approximate model based on experiences</li> <li>solve for values as if the learned model were correct</li></ul> <p>现根据经验构建模型，再使用问题求解方法去计算当前模型</p> <p>就像一个CSP问题我们不知道联系，得先建立相邻状态联系</p> <p>step1：learn empirical MDP model</p> <ul><li>为每个状态和动作做产出（outcomes）统计</li> <li>常态化评估函数T(s,a,s')</li> <li>每当经历<code>s—a—&gt;s'</code>时计算回报函数R(s,a,s')</li></ul> <p>step2：solve the learned MDP（近似的MDP问题）</p> <ul><li>use value iteration</li> <li>use policy iteration</li> <li>......</li></ul> <p>T和R是未知的，但状态空间和行为空间被分配了，要做的就是收集更多数据，动态改善你的模型，估计T和R函数</p> <p>where the reward function come from</p> <ul><li>depend on the human designer</li></ul> <p>how to calculate T function</p> <ul><li>in a simple example, may just looking at the frequencies（频率）</li></ul> <p>计算概率权值：E（概率x值）</p> <ul><li><p>Known P(A)：E(A) = ΣP(a)*a</p></li> <li><p>Unknown P(A)</p> <ul><li><p>Model Based：E(A) = avg(sum(P(a)*a))</p> <p>以某种策略重新计算概率</p></li> <li><p>Model free：E(A) = (1/N)*sum(a)</p> <p>我们认为各种可能概率是相等的，因为尚未总结出规律</p></li> <li><p>二者区别在于是否按概率加权计算均值</p></li></ul></li></ul> <h3 id="model-free-learning">Model-Free Learning</h3> <h4 id="value-learning">Value Learning</h4> <blockquote><p>Passive Reinforcement Learning</p> <p>我们不担心如何在世界模型中行动，只是观察行动并视图估计此代理的状态值</p></blockquote> <p>Simplified task：policy evaluation</p> <ul><li>input：a fixed policy（遵循某一策略）</li> <li>don't know T(s,a,s')</li> <li>don't know R(s,a,s')</li> <li>goal：learn the state values</li></ul> <p>Direct &amp; Indirect Evaluation</p> <blockquote><p>直接估值和间接估值</p></blockquote> <p>直接估值平均观察到的样本值，直接问这一步会有多少<code>reward</code>，仅仅依据实验出的结果的各状态值</p> <p>如直接对于个节点的可能取值求均值作为其状态值，如对C节点使用四次策略</p> <ul><li><p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向D -1，D退出+10</p> <p>C向A -1，A退出-10</p> <p>那么取均值则为<code>(9+9+9-11)/4=4</code></p></li></ul> <p>不需要对T/R做任何事，求均值就行了，只关注值；这不能达到超精确，但随着数据增加总会愈加接近</p> <p>要做的事很明确：</p> <ul><li>选择一个节点</li> <li>多次使用策略进行扩展</li> <li>对扩展结果进行分析取均</li> <li>对该节点赋值得到<code>V(s)</code></li> <li>更新值和策略</li></ul> <p>这一过程始终没用到T/R函数</p> <ul><li><code>VΠ(s) &lt;-- (1/n)Σsample(i)</code></li></ul> <p>注意这里所有的<code>V(s')</code>都应乘上一个<code>λ(&lt;=1)</code>作为时间惩罚（贬值）</p> <p>Temporal difference learning：</p> <ul><li><code>sample = R(s,Π(s), s') + λV(s')</code></li> <li><code>VΠ(s) &lt;-- (1-a)VΠ(s) + (a)sample</code></li></ul> <p>以上为更新已走过节点的方法</p> <p>每次获得新的sample，都对刚走过的状态<code>s</code>进行更新，以接近精确值</p> <p>在这一过程中，我们从未建立世界模型，即T/R函数，只是根据样例值不断更新状态值，随着时间的推移，将得到精确值</p> <p>优化求均值的方法，让越接近的经历比以前的经历更重要，因为我们后来计算的结果总是更加准确</p> <ul><li><p><code>xn = (xn + (1-a)*xn-1 + (1-a)^2*xn-2+...) / 1+(1-a)+(1-a)^2+...</code></p> <p>xn为第n个样例</p></li> <li><p>这里的a为学习率，应用于迭代方程中</p></li></ul> <p>由于我们从未构建模型，也没有T/R函数，根本无从进行策略迭代</p> <p>为什么不学习<code>Q-Value</code>而是<code>V-Value</code>？</p> <p>没有理由，他不仅同样能实现更新Value，而且可以用于策略更新，属于积极的学习</p> <h4 id="q-learning">Q-Learning</h4> <blockquote><p>Active Reinforcement Learning</p> <p>担心数据从何处收集，担心采取行动</p></blockquote> <p>also</p> <ul><li>don't know the transitions T</li> <li>don't know the reward R</li> <li>choose the actions now（当前做的）</li> <li>goal：learn the optimal policy/values</li></ul> <p>不同于MDPs，这不是离线测试（毕竟不知道T/R，无法进行推测），而是真切地采取行动</p> <p>iteration</p> <ul><li>从一个确定状态值开始</li> <li>计算该状态值下一层每个状态的Q-Value和Value</li> <li>通过下一层的Q-Value/Value更新该层的Q-Value/Value</li> <li>迭代这一过程，更新所有Q-Value/Value</li></ul> <p>Value Iteration</p> <ul><li><code>Vk+1(s) &lt;-- maxΣT(s,a,s')[R(s,a,s') + λVk(s')]</code></li></ul> <p>Q-Value Iteration</p> <ul><li><code>Qk+1(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQk(s',a')]</code></li></ul> <p>在这里使用的样例和更新策略</p> <ul><li><p><code>sample = R(s,a,s') + λmaxQk(s',a')</code></p></li> <li><p><code>Q(s,a) &lt;-- (1-a)Q(s,a) + (a)sample</code></p> <p>这个常量a称为学习率</p></li></ul> <p>举例：crawler bot（爬虫机器人）</p> <p>Q-Learning is called off-policy learning</p> <p>Caveats（警告）</p> <ul><li>have to explore enough</li> <li>have to eventually make the learning rate small enough（收敛）</li> <li>...but not decrease it too quickly</li> <li>it doesn't matter how you select actions</li></ul> <table><thead><tr><th>Problem</th> <th>Goal</th> <th>Technique</th></tr></thead> <tbody><tr><td>Known MDP</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Value/Policy iteration; Policy evaluation</td></tr> <tr><td>Unknown MDP: Model-Based</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>VI/PI on approximate MDP; PE on approximate MDP</td></tr> <tr><td>Unknown MDP: Model-Free</td> <td>Compute<code>V*,Q*,Π*</code>; Evaluate a fixed policy</td> <td>Q-learning; Value learning</td></tr></tbody></table> <p>均使用贝尔曼方程进行递归计算</p> <p>Exploration（探索）vs. exploitation（开发）</p> <p><strong>Epsilon Greedy</strong></p> <p>Exploration function</p> <ul><li><p>探索未知节点，收集更多经验：random actions（ε epsilon-greedy）</p> <p>当ε越大，随机度越高，当为0，策略确定</p></li> <li><p>探索方程将根据一个节点的“经验”，如访问过多少次，来给予相应的奖励（访问越多，奖励越低）</p></li> <li><p><code>f(u,n) = u + k/n</code>（基数+奖励/访问次数）</p></li> <li><p>这样能有效腐烂一些无用的节点（越多访问奖励越少）</p></li></ul> <p>Q-Update：加入探索方程</p> <ul><li><p>Regular Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxQ(s',a')]</code></p></li> <li><p>Modified Q-Update:</p> <p><code>Q(s,a) &lt;-- ΣT(s,a,s')[R(s,a,s') + λmaxf(Q(s',a'),N(s',a'))]</code></p></li></ul> <p>Regret</p> <h4 id="approximate-q-learning">Approximate Q-Learning</h4> <p>在实际问题中，状态数、动作会很多很多，很难在Q-Table中去储存每一个Q-Value，这个时候只能做估计</p> <p>w为权重，f为特征值（features）</p> <ul><li><code>V(s) = w1*f1(s)+w2*f2(s)+...+wn*fn(s)</code></li> <li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li></ul> <p>你的Q值将是很多经验的加权和，如f1为跳楼的特征值，f2为纵火的特征值，Q将这些情况的经验汇总以某些权重组合</p> <p>当特征值<code>&gt;1</code>说明他鼓励这种差异，反之对差异持消极态度</p> <p>a仍是学习率</p> <ul><li><p><code>Q(s,a) &lt;-- Q(s,a) + a[diff]</code></p> <p>准确的Q值</p></li> <li><p><code>wi &lt;-- wi + a[diff]f(s,a)</code></p> <p>近似的Q值</p></li></ul> <p>当权重降低，其对应的多项式变低，Q得到调整，那么更新权重成为现在的问题</p> <p>这么做的目的无非是想用相对少的数据得到一个相对好的Q函数</p> <p><strong>Optimization</strong></p> <p>最小二乘法处理特征值<code>features</code></p> <ul><li><code>Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)</code></li> <li><code>Q(s,a)=w0 + w1f1(s,a)</code></li></ul> <p>Minimizing Error</p> <ul><li><code>error(w) = (1/2)*(y-Σwk*fk(x))½</code></li></ul> <p>对该函数对w求导得</p> <ul><li><code>-(y-Σwk*fk(x))fm(x)</code></li></ul> <p>Why limiting capacity can help?</p> <p>功能越多并不一定越好，这意味着更高阶的多项式，在函数曲线上更加符合</p> <p>这有可能造成过度拟合（overfitting），即为了满足一些离谱的数据，做出疯狂的拟合</p> <h4 id="policy-search">Policy Search</h4> <p>尝试不同的策略，看哪一个更好</p> <p><code>Q-Learning</code>：Q值接近，无法确定这是最好的行动</p> <p>让我们关注行动</p> <p>我们有一些Qvalue，向上向下调整特征值权重，看看有什么变化，好则接收，坏则丢弃，然后继续调整，就像CSP的本地搜索</p> <blockquote><p>直升飞机倒挂着飞会省四倍阻力</p> <p>ai vs. ai and train each other</p></blockquote></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">6/23/2024, 12:12:36 AM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev"><a href="/docs/sec/fml/cs188/cs188-search-csp-game.html" class="prev">
          搜索 - 约束满足 - 博弈
        </a></span> <span class="next"><a href="/docs/sec/fml/cs188/cs188-probabilistic-reasoning.html">
          不确定知识 - 概率推理
        </a></span></p></div> <div class="comments-wrapper"><!----></div></main></div> <!----></div> <ul class="sub-sidebar sub-sidebar-wrapper" style="width:12rem;" data-v-b57cc07c data-v-7dd95ae2><li class="level-2" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#mdps" class="sidebar-link reco-side-mdps" data-v-b57cc07c>MDPs</a></li><li class="level-3" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#what-is-mdps" class="sidebar-link reco-side-what-is-mdps" data-v-b57cc07c>What is MDPs</a></li><li class="level-3" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#solving-mdps" class="sidebar-link reco-side-solving-mdps" data-v-b57cc07c>Solving MDPs</a></li><li class="level-3" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#summary" class="sidebar-link reco-side-summary" data-v-b57cc07c>Summary</a></li><li class="level-2" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#rl" class="sidebar-link reco-side-rl" data-v-b57cc07c>RL</a></li><li class="level-3" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#model-based-learning" class="sidebar-link reco-side-model-based-learning" data-v-b57cc07c>Model-Based Learning</a></li><li class="level-3" data-v-b57cc07c><a href="/docs/sec/fml/cs188/cs188-mdp-rl.html#model-free-learning" class="sidebar-link reco-side-model-free-learning" data-v-b57cc07c>Model-Free Learning</a></li></ul></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div><div class="reco-bgm-panel" data-v-b1d3339e><audio id="bgm" src="/song/AmnesiaButterfly.mp3" data-v-b1d3339e></audio> <div class="reco-float-box" style="bottom:44px;z-index:999999;display:none;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><img src="/img/error.jpg" data-v-b1d3339e></div> <div class="reco-bgm-box" style="left:10px;bottom:10px;z-index:999999;" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="reco-bgm-cover" style="background-image:url(/img/error.jpg);" data-v-b1d3339e><div class="mini-operation" style="display:none;" data-v-b1d3339e><i class="reco-bgm reco-bgm-pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play" style="display:none;" data-v-b1d3339e></i></div> <div class="falut-message" style="display:none;" data-v-b1d3339e>
          播放失败
        </div></div> <div class="reco-bgm-info" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-music music" data-v-b1d3339e></i>失忆蝴蝶</div> <div class="info-box" data-v-b1d3339e><i class="reco-bgm reco-bgm-artist" data-v-b1d3339e></i>Eason</div> <div class="reco-bgm-progress" data-v-b1d3339e><div class="progress-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div> <div class="reco-bgm-operation" data-v-b1d3339e><i class="reco-bgm reco-bgm-last last" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-pause pause" style="display:none;" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-play play" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-next next" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-volume1 volume" data-v-b1d3339e></i> <i class="reco-bgm reco-bgm-mute mute" style="display:none;" data-v-b1d3339e></i> <div class="volume-bar" data-v-b1d3339e><div class="bar" data-v-b1d3339e></div></div></div></div> <div class="reco-bgm-left-box" data-v-b1d3339e data-v-41bcba48 data-v-b1d3339e><i class="reco-bgm reco-bgm-left" data-v-b1d3339e></i></div></div></div><!----><div class="RibbonAnimation"></div><div class="Sakura" data-v-248d85d6><canvas id="canvas_sakura" style="z-index:99;" data-v-248d85d6></canvas></div><div class="cat-container" data-v-a13867c0><canvas id="vuepress-cat" width="280" height="250" class="live2d" data-v-a13867c0></canvas></div><canvas id="vuepress-canvas-cursor"></canvas></div></div>
    <script src="/assets/js/app.314cf608.js" defer></script><script src="/assets/js/10.141c7d77.js" defer></script><script src="/assets/js/1.e7a7181a.js" defer></script><script src="/assets/js/176.af70a6e0.js" defer></script>
  </body>
</html>
